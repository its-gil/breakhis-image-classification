{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPEn69Tjh4iK"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36399,
     "status": "ok",
     "timestamp": 1734187450154,
     "user": {
      "displayName": "Michał Wójcik 2",
      "userId": "13864076191058782159"
     },
     "user_tz": 0
    },
    "id": "HqSXHt3ByJLb",
    "outputId": "fd12c366-f763-4014-9d1a-c7ddae039411"
   },
   "outputs": [],
   "source": [
    "# various imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# setting seaborn display pattern for matplotlib\n",
    "sns.set()\n",
    "\n",
    "# sklearn for preprocessing and model evaluation\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "# for faster data loading\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# tensorflow for model structure\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    GlobalAveragePooling2D, Lambda, Dense, BatchNormalization,\n",
    "    Dropout, Concatenate, Activation\n",
    ")\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# mount drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVyaea7Sh81x"
   },
   "source": [
    "### Manipulating the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "executionInfo": {
     "elapsed": 1481,
     "status": "ok",
     "timestamp": 1734187451632,
     "user": {
      "displayName": "Michał Wójcik 2",
      "userId": "13864076191058782159"
     },
     "user_tz": 0
    },
    "id": "xjdh1aWZK67T",
    "outputId": "53344514-ef39-431c-ec8e-b856e06c4326"
   },
   "outputs": [],
   "source": [
    "# load Dataframe\n",
    "imageDataframe = pd.read_csv('../dataset_index/processed_image_data.csv')\n",
    "\n",
    "# create Labelencoder and transform the 'Cancer Type' column\n",
    "classEncoder = LabelEncoder()\n",
    "imageDataframe['Cancer Type Encoded'] = classEncoder.fit_transform(imageDataframe['Cancer Type'])\n",
    "\n",
    "# compute the counts for each original class\n",
    "counts = imageDataframe['Cancer Type'].value_counts()\n",
    "\n",
    "# create a DataFrame for class encodings\n",
    "class_df = pd.DataFrame({\n",
    "    'Decoded': classEncoder.classes_,\n",
    "    'Encoded': range(len(classEncoder.classes_)),\n",
    "    'Counts': [counts[label] for label in classEncoder.classes_]\n",
    "})\n",
    "\n",
    "# sort the DataFrame by the 'Encoded' column\n",
    "class_df = class_df.sort_values(by='Encoded')\n",
    "class_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1734187451632,
     "user": {
      "displayName": "Michał Wójcik 2",
      "userId": "13864076191058782159"
     },
     "user_tz": 0
    },
    "id": "p8e6aYwzybyy",
    "outputId": "378bbb76-8098-4ae4-8508-f579dd0e4106"
   },
   "outputs": [],
   "source": [
    "# dropping the maginfication Collum\n",
    "imageDataframe.drop(columns=['Benign or Malignant', 'Magnification'], inplace = True)\n",
    "\n",
    "imageDataframe = imageDataframe[['rel_path', 'Cancer Type Encoded']]\n",
    "\n",
    "file_paths = imageDataframe.copy()\n",
    "file_paths.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O2n5zZJ5y0Ba"
   },
   "source": [
    "### Preloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 274616,
     "status": "ok",
     "timestamp": 1734187726242,
     "user": {
      "displayName": "Michał Wójcik 2",
      "userId": "13864076191058782159"
     },
     "user_tz": 0
    },
    "id": "u3HRKPaEqfVC",
    "outputId": "b3b1ec66-652c-41f8-e422-2db1240c5b05"
   },
   "outputs": [],
   "source": [
    "shutil.copytree('../PATH/TO/images_plain', '/tmp', dirs_exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AahP-z2iU24"
   },
   "source": [
    "### Building the dataflow generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Rn3S79IAdXO"
   },
   "outputs": [],
   "source": [
    "# parameters\n",
    "BATCH_SIZE = 128\n",
    "IMG_SIZE = (256, 256)\n",
    "\n",
    "# split the data into train, validation, and test sets\n",
    "def split_dataset(filepaths_df):\n",
    "    train_df, temp_df = train_test_split(\n",
    "        filepaths_df,\n",
    "        test_size=0.3,\n",
    "        stratify=filepaths_df['Cancer Type Encoded'],\n",
    "        random_state=42\n",
    "    )\n",
    "    test_df, val_df = train_test_split(\n",
    "        temp_df,\n",
    "        test_size=0.5,\n",
    "        stratify=temp_df['Cancer Type Encoded'],\n",
    "        random_state=42\n",
    "    )\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# validate file paths\n",
    "def validate_file_paths(filepaths_df):\n",
    "    invalid_paths = [path for path in filepaths_df['rel_path'] if not os.path.exists(path)]\n",
    "    if invalid_paths:\n",
    "        raise FileNotFoundError(f\"The following files are missing: {invalid_paths}\")\n",
    "    return True\n",
    "\n",
    "# custom real-time loading function\n",
    "def create_dataset(filepaths_df, img_size, batch_size, augment_labels = None, is_training = True):\n",
    "    file_paths = filepaths_df['rel_path'].values\n",
    "    labels = filepaths_df['Cancer Type Encoded'].astype(np.int32).values\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "\n",
    "    # load image\n",
    "    def load_image(file_path):\n",
    "        image = tf.io.read_file(file_path)\n",
    "        image = tf.image.decode_jpeg(image, channels = 3)\n",
    "        image = tf.image.resize(image, img_size)\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "        return image\n",
    "\n",
    "    # simplified augmentation\n",
    "    def augment_image(image):\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_flip_up_down(image)\n",
    "        return image\n",
    "\n",
    "    # processing images\n",
    "    def process_image(file_path, label):\n",
    "        image = load_image(file_path)\n",
    "\n",
    "        # universal preprocessing, rotate 90 degrees\n",
    "        image = tf.image.rot90(image, tf.random.uniform(shape = [], minval = 0, maxval = 4, dtype = tf.int32))\n",
    "\n",
    "        # conditionally augment the image, enhancing the minority classes\n",
    "        if is_training and augment_labels and tf.reduce_any(tf.equal(label, augment_labels)):\n",
    "            augmented_image = augment_image(image)\n",
    "            return tf.data.Dataset.from_tensors((image, label)).concatenate(\n",
    "                tf.data.Dataset.from_tensors((augmented_image, label))\n",
    "            )\n",
    "        else:\n",
    "            return tf.data.Dataset.from_tensors((image, label))\n",
    "\n",
    "    # apply processing to dataset\n",
    "    dataset = dataset.flat_map(process_image)\n",
    "\n",
    "    # shuffle training set\n",
    "    # and repeat the last datapoints if the batch is too small; used to eliminate a warning that appeared repeatedly\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size = 1000).repeat()\n",
    "\n",
    "    dataset = dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# compute class weights\n",
    "def calculate_class_weights(train_df):\n",
    "    per_class_count = train_df['Cancer Type Encoded'].value_counts()\n",
    "\n",
    "    # multiply counts of classes other than class 1 by 2\n",
    "    for label, count in per_class_count.items():\n",
    "        if label != 1:\n",
    "            per_class_count[label] = count * 2\n",
    "\n",
    "    # calculate class weights\n",
    "    total_samples = per_class_count.sum()\n",
    "    class_weights = {}\n",
    "    for label, count in per_class_count.items():\n",
    "        class_weights[label] = total_samples / (len(per_class_count) * count)\n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOsmmSXuligA"
   },
   "source": [
    "### Executing the creation of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1356,
     "status": "ok",
     "timestamp": 1734187727595,
     "user": {
      "displayName": "Michał Wójcik 2",
      "userId": "13864076191058782159"
     },
     "user_tz": 0
    },
    "id": "OKJJKgKwqMA4",
    "outputId": "5a50e44d-03e3-4b06-f810-4b4bb9f3265b"
   },
   "outputs": [],
   "source": [
    "filepaths = file_paths.copy()\n",
    "\n",
    "# we split the dataset\n",
    "train_df, val_df, test_df = split_dataset(filepaths)\n",
    "\n",
    "# we check that there are no missing files\n",
    "validate_file_paths(train_df)\n",
    "validate_file_paths(val_df)\n",
    "validate_file_paths(test_df)\n",
    "\n",
    "# define augmented labels, everything except D-Carcinoma\n",
    "augment_labels = [0, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# create train, test, validation datasets\n",
    "train_ds = create_dataset(train_df, IMG_SIZE, BATCH_SIZE, augment_labels, is_training=True)\n",
    "val_ds = create_dataset(val_df, IMG_SIZE, BATCH_SIZE, augment_labels, is_training=False).repeat()\n",
    "test_ds = create_dataset(test_df, IMG_SIZE, BATCH_SIZE, augment_labels, is_training=False)\n",
    "\n",
    "# compute effective dataset size\n",
    "augmented_count = train_df['Cancer Type Encoded'].isin(augment_labels).sum()\n",
    "effective_train_size = len(train_df) + augmented_count\n",
    "\n",
    "# calculate steps per epoch\n",
    "steps_per_epoch = effective_train_size // BATCH_SIZE\n",
    "if effective_train_size % BATCH_SIZE != 0:\n",
    "    steps_per_epoch += 1\n",
    "\n",
    "# calculate class weights\n",
    "class_weights = calculate_class_weights(train_df)\n",
    "\n",
    "print(\"Class Weights\")\n",
    "print(class_weights)\n",
    "\n",
    "print(\"\\nSteps_per_epoch\")\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lnik0oSTjSzW"
   },
   "source": [
    "### Defining callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbPkYjibKLxo"
   },
   "outputs": [],
   "source": [
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    min_delta = 0,\n",
    "    patience = 7,\n",
    "    mode = 'auto',\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor = 'val_loss',\n",
    "    factor = 0.5,\n",
    "    patience = 3,\n",
    "    min_lr = 1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAaGbmgijYcJ"
   },
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4962,
     "status": "ok",
     "timestamp": 1734187732555,
     "user": {
      "displayName": "Michał Wójcik 2",
      "userId": "13864076191058782159"
     },
     "user_tz": 0
    },
    "id": "pg6k15VWzbmT",
    "outputId": "2ad063da-0db7-4697-94fd-974d76d2f433"
   },
   "outputs": [],
   "source": [
    "# the base model is a pretrained DenseNet121\n",
    "base_model = DenseNet121(\n",
    "    weights='imagenet',\n",
    "    include_top=False,\n",
    "    input_shape = (256, 256, 3)\n",
    ")\n",
    "\n",
    "# unfreeze all the layers\n",
    "base_model.trainable = True\n",
    "\n",
    "# find the last convolutional layer...\n",
    "for layer in base_model.layers:\n",
    "    if layer.name == 'conv5_block16_concat':\n",
    "        x = layer.output\n",
    "        break\n",
    "\n",
    "# and change the rest of the model to combat overfitting\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# flatten and normalize\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Lambda(lambda x: tf.math.l2_normalize(x, axis = 1))(x)\n",
    "\n",
    "# final layers\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "predictions = Dense(8, activation='softmax')(x)\n",
    "\n",
    "# creating the model\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# compiling the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1445,
     "status": "ok",
     "timestamp": 1734187733996,
     "user": {
      "displayName": "Michał Wójcik 2",
      "userId": "13864076191058782159"
     },
     "user_tz": 0
    },
    "id": "flEWmp9rsSWH",
    "outputId": "5a4263e3-591d-4ef6-f662-a21b0bec39dc"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmGQzWaXjbXo"
   },
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4195963,
     "status": "ok",
     "timestamp": 1734191929955,
     "user": {
      "displayName": "Michał Wójcik 2",
      "userId": "13864076191058782159"
     },
     "user_tz": 0
    },
    "id": "r42hNxdDznbV",
    "outputId": "612e31bf-f9ba-4ca5-e3e1-13d7bae4ab76"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data = val_ds,\n",
    "    epochs = 70,\n",
    "    verbose = 1,\n",
    "\n",
    "    # we use the calculations made above\n",
    "    class_weight = class_weights,\n",
    "\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    validation_steps = len(val_df) // BATCH_SIZE,\n",
    "    callbacks = [earlystopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rbIMxOv4jggF"
   },
   "source": [
    "### Comparing the training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 817,
     "status": "ok",
     "timestamp": 1734191930766,
     "user": {
      "displayName": "Michał Wójcik 2",
      "userId": "13864076191058782159"
     },
     "user_tz": 0
    },
    "id": "TOTy7tYrlZs2",
    "outputId": "683d2e34-d8a8-4cfd-f9b1-7d0e51b596d7"
   },
   "outputs": [],
   "source": [
    "# plotting the validation and the training loss and accuracy in two plots, side by side\n",
    "plt.figure(figsize = (12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc = 'upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0yyWzmPtjhyo"
   },
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37616,
     "status": "ok",
     "timestamp": 1734191968377,
     "user": {
      "displayName": "Michał Wójcik 2",
      "userId": "13864076191058782159"
     },
     "user_tz": 0
    },
    "id": "HRPXSWCT13fw",
    "outputId": "61c444fb-f583-469d-8c42-78c1a866c92c"
   },
   "outputs": [],
   "source": [
    "# get true labels from the test dataset\n",
    "y_test_true = np.concatenate([y.numpy() for x, y in test_ds], axis=0)\n",
    "test_predictions = model.predict(test_ds)\n",
    "\n",
    "# predicting on the test dataset and rounding the results\n",
    "y_test_pred = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# print the F1 score, accuracy, and classification report\n",
    "test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "test_accuracy = np.mean(y_test_true == y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test F1 Score:\", test_f1)\n",
    "print(classification_report(y_test_true, y_test_pred))\n",
    "\n",
    "# compute and display the confusion matrix\n",
    "test_conf_matrix = confusion_matrix(y_test_true, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(test_conf_matrix, annot=True, fmt = 'd', cmap = 'Blues', xticklabels = np.unique(y_test_true), yticklabels = np.unique(y_test_true))\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - Test Data\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "FPEn69Tjh4iK",
    "NVyaea7Sh81x",
    "O2n5zZJ5y0Ba",
    "6AahP-z2iU24",
    "AOsmmSXuligA",
    "Lnik0oSTjSzW",
    "FAaGbmgijYcJ",
    "MmGQzWaXjbXo",
    "rbIMxOv4jggF",
    "0yyWzmPtjhyo"
   ],
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1Qh2no82wIDp-wH_dL8h4Rbv7TXwyisAB",
     "timestamp": 1731670822197
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
