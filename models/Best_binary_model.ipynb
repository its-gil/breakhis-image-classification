{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-dUs58EsCzz"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36599,
     "status": "ok",
     "timestamp": 1734187458149,
     "user": {
      "displayName": "Michał Wójcik",
      "userId": "08591631202502653658"
     },
     "user_tz": 0
    },
    "id": "HqSXHt3ByJLb",
    "outputId": "bcd93ada-df4d-43d5-d7e2-6179116a5ffc"
   },
   "outputs": [],
   "source": [
    "# various imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# setting Seaborn display pattern for matplotlib\n",
    "sns.set()\n",
    "\n",
    "# sklearn for preprocessing and model evaluation\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
    "\n",
    "# faster data loading\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# model-related imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import (\n",
    "    GlobalAveragePooling2D, Lambda, Dense, BatchNormalization, Activation,\n",
    "    Dropout, Concatenate\n",
    ")\n",
    "\n",
    "# mount drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSU8LqwRsHHI"
   },
   "source": [
    "### Manipulating the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745
    },
    "executionInfo": {
     "elapsed": 958,
     "status": "ok",
     "timestamp": 1734187459104,
     "user": {
      "displayName": "Michał Wójcik",
      "userId": "08591631202502653658"
     },
     "user_tz": 0
    },
    "id": "p8e6aYwzybyy",
    "outputId": "ddb96518-961c-499d-c241-146eb0edb44f"
   },
   "outputs": [],
   "source": [
    "imageDataframe = pd.read_csv('../dataset_index/processed_image_data.csv')\n",
    "\n",
    "# encode the column of the tumor class - binary => LabelEncoder\n",
    "classEncoder = LabelEncoder()\n",
    "imageDataframe['Benign or Malignant'] = classEncoder.fit_transform(imageDataframe['Benign or Malignant'])\n",
    "\n",
    "counts = imageDataframe['Benign or Malignant'].value_counts()\n",
    "\n",
    "# encode the column of the tumor subclass - multi-class => OneHotEncoder\n",
    "subClassEncoder= OneHotEncoder(sparse_output = False)\n",
    "imageDataframe[subClassEncoder.get_feature_names_out(['Cancer Type'])] = subClassEncoder.fit_transform(imageDataframe[['Cancer Type']])\n",
    "\n",
    "# drop magnification column\n",
    "imageDataframe.drop(columns=['Cancer Type', 'Magnification'], inplace = True)\n",
    "\n",
    "imageDataframe.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 178
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1734187459104,
     "user": {
      "displayName": "Michał Wójcik",
      "userId": "08591631202502653658"
     },
     "user_tz": 0
    },
    "id": "PrB474SfkKp-",
    "outputId": "eb5295e3-bced-4c94-8f3d-4a2510a0a7ba"
   },
   "outputs": [],
   "source": [
    "# check for class counts\n",
    "imageDataframe.value_counts('Benign or Malignant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1734187459104,
     "user": {
      "displayName": "Michał Wójcik",
      "userId": "08591631202502653658"
     },
     "user_tz": 0
    },
    "id": "u3HRKPaEqfVC",
    "outputId": "36da1612-52a2-41b0-de92-a166bcddd35f"
   },
   "outputs": [],
   "source": [
    "imageDataframe = imageDataframe[['rel_path', 'Benign or Malignant']]\n",
    "\n",
    "file_paths = imageDataframe.copy()\n",
    "file_paths.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J7WfyV_MsSUE"
   },
   "source": [
    "### Copy images into the tmp for faster loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 212521,
     "status": "ok",
     "timestamp": 1734187671617,
     "user": {
      "displayName": "Michał Wójcik",
      "userId": "08591631202502653658"
     },
     "user_tz": 0
    },
    "id": "TRtv50amkhkb",
    "outputId": "9516437a-33f2-407a-a016-53a34bb543b6"
   },
   "outputs": [],
   "source": [
    "# This works in Google Colab\n",
    "shutil.copytree('../PATH/TO/images_plain', '/tmp', dirs_exist_ok = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyHygQEHr9Ml"
   },
   "source": [
    "### Methods for creating the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "1Rn3S79IAdXO"
   },
   "outputs": [],
   "source": [
    "# fixed parameters\n",
    "BATCH_SIZE = 128\n",
    "IMG_SIZE = (256, 256)\n",
    "\n",
    "# method for splitting the data into train, validation, and test sets\n",
    "def split_dataset(filepaths_df):\n",
    "\n",
    "    # 70% training\n",
    "    train_df, temp_df = train_test_split(\n",
    "        filepaths_df,\n",
    "        test_size = 0.3,\n",
    "        stratify = filepaths_df['Benign or Malignant'],\n",
    "        random_state = 0\n",
    "    )\n",
    "\n",
    "    # 15% validation, 15% testing\n",
    "    test_df, val_df = train_test_split(\n",
    "        temp_df,\n",
    "        test_size = 0.5,\n",
    "        stratify = temp_df['Benign or Malignant'],\n",
    "        random_state = 0\n",
    "    )\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "# we check if files are missing\n",
    "def validate_file_paths(filepaths_df):\n",
    "    invalid_paths = [path for path in filepaths_df['rel_path'] if not os.path.exists(path)]\n",
    "    if invalid_paths:\n",
    "        raise FileNotFoundError(f\"The following files are missing: {invalid_paths}\")\n",
    "    return True\n",
    "\n",
    "# creating the dataset with augmentation, batch and prefetch\n",
    "def create_dataset(filepaths_df, img_size, batch_size, augment_labels = None, is_training = True):\n",
    "    file_paths = filepaths_df['rel_path'].values\n",
    "    labels = filepaths_df['Benign or Malignant'].astype(np.int32).values\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((file_paths, labels))\n",
    "\n",
    "    # load images\n",
    "    def load_image(file_path):\n",
    "\n",
    "        # read images\n",
    "        image = tf.io.read_file(file_path)\n",
    "        image = tf.image.decode_jpeg(image, channels=3)\n",
    "\n",
    "        # we scale down the images\n",
    "        image = tf.image.resize(image, img_size)\n",
    "\n",
    "        # normalize images\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "\n",
    "        return image\n",
    "\n",
    "    # simplified augmentation without keras.Sequential\n",
    "    # we flip and change the brightness, contrast and saturation\n",
    "    def augment_image(image):\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_flip_up_down(image)\n",
    "        image = tf.image.adjust_brightness(image, delta = 0.1)\n",
    "        image = tf.image.random_contrast(image, lower = 0.8, upper = 1.2)\n",
    "        image = tf.image.random_saturation(image, lower = 0.8, upper = 1.2)\n",
    "        return image\n",
    "\n",
    "    # processing images\n",
    "    def process_image(file_path, label):\n",
    "        image = load_image(file_path)\n",
    "\n",
    "        # we rotate all the images by 90 degrees\n",
    "        image = tf.image.rot90(image, tf.random.uniform(shape = [], minval = 0, maxval = 4, dtype=tf.int32))\n",
    "\n",
    "        # check if the label has to be augmented\n",
    "        # if so, augment and return dataset\n",
    "        if is_training and augment_labels and tf.reduce_any(tf.equal(label, augment_labels)):\n",
    "            augmented_image = augment_image(image)\n",
    "            return tf.data.Dataset.from_tensors((image, label)).concatenate(\n",
    "                    tf.data.Dataset.from_tensors((augmented_image, label))\n",
    "            )\n",
    "        else:\n",
    "            return tf.data.Dataset.from_tensors((image, label))\n",
    "\n",
    "    # we remove nesting\n",
    "    dataset = dataset.flat_map(process_image)\n",
    "\n",
    "    # shuffle training set\n",
    "    # and repeat the last datapoints if the batch is too small; used to eliminate a warning that appeared repeatedly\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(buffer_size = 1000).repeat()\n",
    "\n",
    "    # batch and prefetch dataset\n",
    "    dataset = dataset.batch(batch_size).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# computing the class weights to handle class imbalance\n",
    "# a dictionary of class weights is returned\n",
    "def calculate_class_weights(train_df):\n",
    "    per_class_count = train_df['Benign or Malignant'].value_counts()\n",
    "\n",
    "    # multiply counts of class 0 by 2\n",
    "    for label, count in per_class_count.items():\n",
    "        if label != 1:\n",
    "            per_class_count[label] = count * 2\n",
    "\n",
    "    # calculate class weights\n",
    "    total_samples = per_class_count.sum()\n",
    "    class_weights = {}\n",
    "    for label, count in per_class_count.items():\n",
    "        class_weights[label] = total_samples / (len(per_class_count) * count)\n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1t9yyFMrAUb"
   },
   "source": [
    "### Executing the creation of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1772,
     "status": "ok",
     "timestamp": 1734187673385,
     "user": {
      "displayName": "Michał Wójcik",
      "userId": "08591631202502653658"
     },
     "user_tz": 0
    },
    "id": "5rrFVDpFqayi",
    "outputId": "df36456a-dc0f-4fe2-8d43-c7ba3f8518a2"
   },
   "outputs": [],
   "source": [
    "filepaths = file_paths.copy()\n",
    "\n",
    "# we split the dataset\n",
    "train_df, val_df, test_df = split_dataset(filepaths)\n",
    "\n",
    "# we check that there are no missing files\n",
    "validate_file_paths(train_df)\n",
    "validate_file_paths(val_df)\n",
    "validate_file_paths(test_df)\n",
    "\n",
    "# we define the minority class\n",
    "augment_labels = [0]\n",
    "\n",
    "# creating the datasets\n",
    "train_ds = create_dataset(train_df, IMG_SIZE, BATCH_SIZE, augment_labels, is_training=True)\n",
    "val_ds = create_dataset(val_df, IMG_SIZE, BATCH_SIZE, augment_labels, is_training=False).repeat()\n",
    "test_ds = create_dataset(test_df, IMG_SIZE, BATCH_SIZE, augment_labels, is_training=False)\n",
    "\n",
    "# compute effective dataset size after augmentations\n",
    "augmented_count = train_df['Benign or Malignant'].isin(augment_labels).sum()\n",
    "effective_train_size = len(train_df) + augmented_count\n",
    "\n",
    "# calculate steps per epoch\n",
    "steps_per_epoch = effective_train_size // BATCH_SIZE\n",
    "if effective_train_size % BATCH_SIZE != 0:\n",
    "    steps_per_epoch += 1\n",
    "\n",
    "# calculating the actual class weights\n",
    "class_weights = calculate_class_weights(train_df)\n",
    "\n",
    "print(\"Class Weights\")\n",
    "print(class_weights)\n",
    "\n",
    "print(\"\\nSteps_per_epoch\")\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hx5mr3tgzkDX"
   },
   "source": [
    "### Defining callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EbPkYjibKLxo"
   },
   "outputs": [],
   "source": [
    "earlystopping = keras.callbacks.EarlyStopping(\n",
    "    monitor = 'val_loss',\n",
    "    min_delta = 0,\n",
    "    patience = 7,\n",
    "    mode = 'auto',\n",
    "    restore_best_weights = True\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor = 0.5,\n",
    "    patience = 3,\n",
    "    min_lr = 1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twbWEu0PtM2m"
   },
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4190,
     "status": "ok",
     "timestamp": 1734187677572,
     "user": {
      "displayName": "Michał Wójcik",
      "userId": "08591631202502653658"
     },
     "user_tz": 0
    },
    "id": "pg6k15VWzbmT",
    "outputId": "716a35c8-281f-43dd-d804-42d81cf0f51d"
   },
   "outputs": [],
   "source": [
    "# the base model is a pretrained DenseNet121\n",
    "base_model = DenseNet121(\n",
    "    weights = 'imagenet',\n",
    "    include_top = False,\n",
    "    input_shape = (256, 256, 3)\n",
    ")\n",
    "\n",
    "# unfreeze all the layers\n",
    "base_model.trainable = True\n",
    "\n",
    "# find the last convolutional layer...\n",
    "for layer in base_model.layers:\n",
    "    if layer.name == 'conv5_block16_concat':\n",
    "        x = layer.output\n",
    "        break\n",
    "\n",
    "# and change the rest of the model to combat overfitting\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "# flatten and normalize\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Lambda(lambda x: tf.math.l2_normalize(x, axis = 1))(x)\n",
    "\n",
    "# final layers\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.5)(x)\n",
    "\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# creating the model\n",
    "model = Model(inputs = base_model.input, outputs = output)\n",
    "\n",
    "# compiling the model\n",
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0001),\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNnxSfLsuQ_F"
   },
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2982716,
     "status": "ok",
     "timestamp": 1734190660285,
     "user": {
      "displayName": "Michał Wójcik",
      "userId": "08591631202502653658"
     },
     "user_tz": 0
    },
    "id": "r42hNxdDznbV",
    "outputId": "436a841b-cc8c-447d-9932-fd8e79b92f7a"
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data = val_ds,\n",
    "    epochs = 50,\n",
    "    verbose = 1,\n",
    "\n",
    "    # we use the calculations made above\n",
    "    steps_per_epoch = steps_per_epoch,\n",
    "    validation_steps = len(val_df) // BATCH_SIZE,\n",
    "    class_weight = class_weights,\n",
    "\n",
    "    callbacks=[earlystopping, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpHRm5LIvDev"
   },
   "source": [
    "### Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyZ1b20eMVgv"
   },
   "outputs": [],
   "source": [
    "# model.save('./YOUR/PATH/Custom_augment_binary.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p5ilaGcry5Q2"
   },
   "source": [
    "### Comparing the training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 1103,
     "status": "ok",
     "timestamp": 1734190661385,
     "user": {
      "displayName": "Michał Wójcik",
      "userId": "08591631202502653658"
     },
     "user_tz": 0
    },
    "id": "xJoyKwww5vPf",
    "outputId": "2decf3e7-b175-454c-9fc8-dff00b0c9c1d"
   },
   "outputs": [],
   "source": [
    "# plotting the validation and the training loss and accuracy in two plots, side by side\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc = 'upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc = 'upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFB8xMqNzncH"
   },
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 836
    },
    "executionInfo": {
     "elapsed": 51348,
     "status": "ok",
     "timestamp": 1734190712729,
     "user": {
      "displayName": "Michał Wójcik",
      "userId": "08591631202502653658"
     },
     "user_tz": 0
    },
    "id": "HRPXSWCT13fw",
    "outputId": "1657989c-f77f-45af-db8f-b0069affd009"
   },
   "outputs": [],
   "source": [
    "# get true labels from the test dataset\n",
    "y_test_true = np.concatenate([y.numpy() for x, y in test_ds], axis = 0)\n",
    "\n",
    "# predicting on the test dataset and rounding the results\n",
    "test_predictions = model.predict(test_ds)\n",
    "y_test_pred = np.round(test_predictions).astype(int).flatten()\n",
    "\n",
    "# print the F1 score, accuracy, and classification report\n",
    "test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "test_accuracy = np.mean(y_test_true == y_test_pred)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "print(\"Test F1 Score:\", test_f1)\n",
    "print(classification_report(y_test_true, y_test_pred))\n",
    "\n",
    "# compute and display the confusion matrix\n",
    "test_conf_matrix = confusion_matrix(y_test_true, y_test_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(test_conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - Test Data\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "b-dUs58EsCzz",
    "vSU8LqwRsHHI",
    "J7WfyV_MsSUE",
    "xyHygQEHr9Ml",
    "y1t9yyFMrAUb",
    "Hx5mr3tgzkDX",
    "twbWEu0PtM2m",
    "DNnxSfLsuQ_F",
    "zpHRm5LIvDev",
    "p5ilaGcry5Q2",
    "JFB8xMqNzncH"
   ],
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1Qh2no82wIDp-wH_dL8h4Rbv7TXwyisAB",
     "timestamp": 1731670822197
    }
   ]
  },
  "kernelspec": {
   "display_name": "dlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
